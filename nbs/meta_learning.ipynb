{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, EvalPrediction, GlueDataset, default_data_collator) \n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import (HfArgumentParser, Trainer, TrainingArguments,\n",
    "                          glue_compute_metrics, glue_output_modes,\n",
    "                          glue_tasks_num_labels, set_seed)\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import default_data_collator\n",
    "import pandas as pd\n",
    "# import higher\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from fluence.meta import MetaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = DataTrainingArguments(task_name = 'MNLI', data_dir = '/home/nlp/data/glue_data/MNLI',\n",
    "                                 max_seq_length=80)\n",
    "training_args = TrainingArguments(output_dir = '/home/nlp/experiments/meta/',\n",
    "                                 do_eval = True,\n",
    "                                 per_device_train_batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130899/130899 [00:10<00:00, 12835.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GlueDataset(data_args, tokenizer=tokenizer)\n",
    "meta_dataset = MetaDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in meta_dataset[0]:\n",
    "    inputs = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MetaTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        args: TrainingArguments,\n",
    "        train_dataloader: DataLoader,\n",
    "        eval_dataloader: DataLoader,\n",
    "        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "        prediction_loss_only=False,\n",
    "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (\n",
    "            None,\n",
    "            None,\n",
    "        ),\n",
    "    ):\n",
    "\n",
    "        self.model = model.to(args.device)\n",
    "        self.args = args\n",
    "        self.compute_metrics = compute_metrics\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        self.prediction_loss_only = prediction_loss_only\n",
    "        self.optimizer, self.lr_scheduler = optimizers\n",
    "        self.epoch = None\n",
    "        self.tb_writer = None\n",
    "        set_seed(self.args.seed)\n",
    "\n",
    "    def run_maml(self):\n",
    "        meta_dataset = self.train_dataloader.dataset\n",
    "        sum_gradients = []\n",
    "        \n",
    "        eval_step = [2 ** i for i in range(1, 20)]\n",
    "        \n",
    "        for task_id in tqdm(2*self.args.max_sample_limit):\n",
    "            \n",
    "            fast_model = deepcopy(self.model)\n",
    "            fast_model.to(self.args.device)\n",
    "            inner_optimizer = torch.optim.AdamW(fast_model.parameters(), lr=self.args.step_size)\n",
    "            fast_model.train()\n",
    "            inner_optimizer.zero_grad()\n",
    "            \n",
    "            # Support set [classes]\n",
    "            for task in meta_dataset[task_id]:\n",
    "                loss = fast_model(**task)[0]\n",
    "                loss.backward()\n",
    "                inner_optimizer.step()\n",
    "                \n",
    "            # Query Set [classes]\n",
    "            for task in meta_dataset[task_id]:\n",
    "                query_loss = fast_model(**task)[0]\n",
    "                query_loss.backward()\n",
    "                fast_model.to(torch.device('cpu'))\n",
    "                for i, params in enumerate(fast_model.parameters()):\n",
    "                    if task_id == 0:\n",
    "                        sum_gradients.append(deepcopy(params.grad))\n",
    "                    else:\n",
    "                        sum_gradients[i] += deepcopy(params.grad)\n",
    "\n",
    "            del fast_model, inner_optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Run evaluation as per eval_step\n",
    "            if self.global_step in eval_step:\n",
    "                output = self.prediction_loop(self.eval_dataloader, description = \"Evaluation\")\n",
    "                self.log(output.metrics)\n",
    "\n",
    "                output_dir = os.path.join(\n",
    "                    self.args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.global_step}\",\n",
    "                )\n",
    "                self.save_model(output_dir)\n",
    "        \n",
    "        # Outer loop\n",
    "        for i in range(0, len(sum_gradients)):\n",
    "                sum_gradients[i] = sum_gradients[i] / float(self.args.max_sample_limit)\n",
    "\n",
    "            for i, params in enumerate(self.model.parameters()):\n",
    "                params.grad = sum_gradients[i]\n",
    "\n",
    "            self.outer_optimizer.step()\n",
    "            self.outer_optimizer.zero_grad()\n",
    "            \n",
    "            del sum_gradients\n",
    "            gc.collect()\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        self.create_optimizer_and_scheduler(self.args.max_sample_limit)\n",
    "\n",
    "        logger.info(\"***** Running training *****\")\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.epoch = 0\n",
    "\n",
    "        eval_step = [2 ** i for i in range(1, 20)]\n",
    "        inner_optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(), lr=self.args.step_size\n",
    "        )\n",
    "        self.model.train()\n",
    "\n",
    "        tqdm_iterator = tqdm(self.train_dataloader, desc=\"Batch Index\")\n",
    "\n",
    "        #  self.model.zero_grad()\n",
    "        self.optimizer.zero_grad()\n",
    "        query_dataloader = iter(self.train_dataloader)\n",
    "\n",
    "        for batch_idx, meta_batch in enumerate(tqdm_iterator):\n",
    "            target_batch = next(query_dataloader)\n",
    "            outer_loss = 0.0\n",
    "            # Loop through all classes\n",
    "            for inputs, target_inputs in zip(meta_batch, target_batch):\n",
    "\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(self.args.device)\n",
    "                    target_inputs[k] = v.to(self.args.device)\n",
    "\n",
    "                with higher.innerloop_ctx(\n",
    "                    self.model, inner_optimizer, copy_initial_weights=False\n",
    "                ) as (fmodel, diffopt):\n",
    "\n",
    "                    inner_loss = fmodel(**inputs)[0]\n",
    "                    diffopt.step(inner_loss)\n",
    "                    outer_loss += fmodel(**target_inputs)[0]\n",
    "\n",
    "            self.global_step += 1\n",
    "            self.optimizer.step()\n",
    "            self.lr_scheduler.step()\n",
    "            outer_loss.backward()\n",
    "            #  self.model.zero_grad()\n",
    "\n",
    "            if (batch_idx + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), self.args.max_grad_norm\n",
    "                )\n",
    "\n",
    "            # Run evaluation on task list\n",
    "            if self.global_step in eval_step:\n",
    "                output = self.prediction_loop(self.eval_dataloader, description = \"Evaluation\")\n",
    "                self.log(output.metrics)\n",
    "\n",
    "                output_dir = os.path.join(\n",
    "                    self.args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.global_step}\",\n",
    "                )\n",
    "                self.save_model(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
