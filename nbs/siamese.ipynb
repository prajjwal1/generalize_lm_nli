{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Optional, Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification, AutoModel,\n",
    "                          AutoTokenizer, PreTrainedTokenizer, PreTrainedModel)\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "from datasets.siamese_dataset import SiameseGlueDataset, siamese_data_collator\n",
    "from models.siamese_model import SiameseTransformer\n",
    "# from models.siamese_pooling import Pooling\n",
    "from core.siamese_trainer import SiameseTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'mnli'\n",
    "data_dir = '/home/nlp/data/glue_data/MNLI'\n",
    "model_id = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DataTrainingArguments(task_name, data_dir = data_dir, max_seq_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_train_dataset = SiameseGlueDataset(args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(siamese_train_dataset[0][0].input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_eval_dataset = SiameseGlueDataset(args, tokenizer, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.siamese_dataset import siamese_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(siamese_train_dataset,\n",
    "                      batch_size=8,\n",
    "                     collate_fn = siamese_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SiameseModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to SiameseTransformer\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = field(\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Path to pretrained model or model identifier from\"\n",
    "                \" huggingface.co/models\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    #input_dim: int = field(\n",
    "    #    default=None, metadata={\"help\": \"Input dimension of linear layer\"}\n",
    "    #)\n",
    "    #linear_dim: int = field(\n",
    "    #    default=None, metadata={\"help\": \"Dimension of linear layer\"}\n",
    "    #)\n",
    "    seq_len: int = field(default = 128)\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Where do you want to store the pretrained models downloaded from s3\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_a: bool = field(default=False, metadata={\"help\": \"freeze model a\"})\n",
    "    freeze_b: bool = field(default=False, metadata={\"help\": \"freeze model b\"})\n",
    "    num_labels: int = field(default=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 128))\n",
    "        self.dense = nn.Linear(4096, len(config.id2label))\n",
    "                \n",
    "    def forward(self, features):\n",
    "        features = self.pool(features)\n",
    "        print(features.shape)\n",
    "        features = features.view(features.shape[0]//4, -1)\n",
    "        features = self.dense(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, args, config):\n",
    "        super(SiameseTransformer, self).__init__()\n",
    "        self.args = args\n",
    "        self.model_a = AutoModel.from_pretrained(self.args.model_name, \n",
    "                           config=config, cache_dir=self.args.cache_dir)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        self.cls = PredictionHead(config)\n",
    "        \n",
    "        if self.args.freeze_a:\n",
    "            logger.info(\"**** Freezing Model A ****\")\n",
    "            for param in self.model_a.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.args.freeze_b:\n",
    "            logger.info(\"**** Freezing Model B ****\")\n",
    "            for param in self.model_b.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_a, input_b):\n",
    "        labels = input_a['labels']\n",
    "        input_a.pop('labels')\n",
    "        input_b.pop('labels')\n",
    "        output_a = self.model_a(**input_a)[0] # [bs, seq_len, 768]\n",
    "        output_b = self.model_a(**input_b)[0]\n",
    "        concat_output = torch.cat([output_a, output_b, (output_a-output_b), (output_a*output_b)])\n",
    "        logits = self.cls(concat_output)\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bert import ACT2FN, BertLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SiameseModelArguments('bert-base-uncased', seq_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels = 3,\n",
    "        task_name = 'MNLI',\n",
    "        cache_dir = '/home/nlp/experiments/siamese'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 128))\n",
    "        self.dense = nn.Linear(4096, len(config.id2label))\n",
    "                \n",
    "    def forward(self, features):\n",
    "        features = self.pool(features)\n",
    "        print(features.shape)\n",
    "        features = features.view(features.shape[0]//4, -1)\n",
    "        features = self.dense(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = PredictionHeadTransform(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.rand(8, 64, 768) # 8, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionHead(\n",
       "  (dense_1): Linear(in_features=109, out_features=768, bias=True)\n",
       "  (dense_2): Linear(in_features=612, out_features=3, bias=True)\n",
       "  (pool1): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (pool2): AvgPool2d(kernel_size=5, stride=5, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseTransformer(args, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in batch['a'].items():\n",
    "    batch['a'][k] = v.cuda()\n",
    "for k, v in batch['b'].items():\n",
    "    batch['b'][k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(batch['a'], batch['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = AutoModel.from_pretrained('bert-base-uncased', \n",
    "                           config=config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['a'].pop('labels')\n",
    "output_a = model_a(**batch['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
