{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "sys.path.append(\"..\")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification, AutoModel,\n",
    "                          AutoTokenizer, PreTrainedTokenizer, PreTrainedModel)\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "from core.siamese_dataset import SiameseGlueDataset, siamese_data_collator\n",
    "from core.siamese_model import SiameseTransformer\n",
    "from core.siamese_trainer import SiameseTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'mnli'\n",
    "data_dir = '/home/nlp/data/glue_data/MNLI'\n",
    "model_id = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DataTrainingArguments(task_name, data_dir = data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_train_dataset = SiameseGlueDataset(args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_eval_dataset = SiameseGlueDataset(args, tokenizer, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SiameseArguments(TrainingArguments):\n",
    "    model_a: str = field(\n",
    "        default = None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Path to pretrained model or model identifier from\"\n",
    "                \" huggingface.co/models\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    model_b: str = field(\n",
    "        default = None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Path to pretrained model or model identifier from\"\n",
    "                \" huggingface.co/models\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    input_dim: int = field(default = None)\n",
    "    linear_dim: int = field(default=None)\n",
    "    num_labels: int = field(default=None)\n",
    "    freeze_a: bool = field(default=None)\n",
    "    freeze_b: bool = field(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SiameseModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to SiameseTransformer\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Path to pretrained model or model identifier from\"\n",
    "                \" huggingface.co/models\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    input_dim: int = field(\n",
    "        default=None, metadata={\"help\": \"Input dimension of linear layer\"}\n",
    "    )\n",
    "    linear_dim: int = field(\n",
    "        default=None, metadata={\"help\": \"Dimension of linear layer\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Where do you want to store the pretrained models downloaded from s3\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_a: bool = field(default=False, metadata={\"help\": \"freeze model a\"})\n",
    "    freeze_b: bool = field(default=False, metadata={\"help\": \"freeze model b\"})\n",
    "    num_labels: int = field(default=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, args, config):\n",
    "        super(SiameseTransformer, self).__init__()\n",
    "        self.args = args\n",
    "        self.model_a = AutoModel.from_pretrained(self.args.model_name_or_path, \n",
    "                           config=config, cache_dir=self.args.cache_dir)\n",
    "        self.model_b = AutoModel.from_pretrained(self.args.model_name_or_path,\n",
    "                           config=config, cache_dir=self.args.cache_dir)\n",
    "        \n",
    "        if self.args.freeze_a:\n",
    "            logger.info(\"**** Freezing Model A ****\")\n",
    "            for param in self.model_a.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.args.freeze_b:\n",
    "            logger.info(\"**** Freezing Model B ****\")\n",
    "            for param in self.model_b.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "        self.linear = nn.Sequential(nn.Linear(self.args.input_dim, self.args.linear_dim),\n",
    "                                    nn.Linear(self.args.linear_dim, self.args.num_labels)\n",
    "                                   )\n",
    "    \n",
    "    def forward(self, input_a, input_b):\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        labels = input_a['labels']\n",
    "        input_a.pop('labels')\n",
    "        input_b.pop('labels')\n",
    "        output_a = self.model_a(**input_a)[0][:, 0, :]\n",
    "        output_b = self.model_b(**input_b)[0][:, 0, :]\n",
    "        concat_output = torch.cat([output_a, output_b])\n",
    "        concat_output = concat_output.view(labels.size(0), -1)\n",
    "        logits = self.linear(concat_output)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SiameseModelArguments('bert-base-uncased', linear_dim=4096, input_dim=1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels = 3,\n",
    "        task_name = 'MNLI',\n",
    "        cache_dir = '/home/nlp/experiments/siamese'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseTransformer(args, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, '/home/nlp/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/home/nlp/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
