{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Optional, Dict, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification, AutoModel,\n",
    "                          AutoTokenizer, PreTrainedTokenizer, PreTrainedModel)\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import TrainingArguments, default_data_collator, EvalPrediction, GlueDataset\n",
    "from datasets.siamese_dataset import SiameseGlueDataset, siamese_data_collator\n",
    "from models.siamese_model import SiameseTransformer\n",
    "from core.siamese_trainer import SiameseTrainer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'mnli'\n",
    "data_dir = '/home/nlp/data/glue_data/MNLI'\n",
    "model_id = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = DataTrainingArguments(task_name, data_dir = data_dir, max_seq_length=128)\n",
    "mm_data_args = DataTrainingArguments('mnli-mm', data_dir = data_dir, max_seq_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_train_dataset = SiameseGlueDataset(data_args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_eval_dataset = SiameseGlueDataset(data_args, tokenizer, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.siamese_dataset import siamese_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(siamese_train_dataset,\n",
    "                      batch_size=256,\n",
    "                     collate_fn = siamese_data_collator, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dl = DataLoader(siamese_train_dataset,\n",
    "                      batch_size=256,\n",
    "                     collate_fn = siamese_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs in train_dl:\n",
    "    k1 = inputs['a']['input_ids']\n",
    "    k2 = inputs['b']['input_ids']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SiameseModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to SiameseTransformer\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = field(\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Path to pretrained model or model identifier from\"\n",
    "                \" huggingface.co/models\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    #input_dim: int = field(\n",
    "    #    default=None, metadata={\"help\": \"Input dimension of linear layer\"}\n",
    "    #)\n",
    "    #linear_dim: int = field(\n",
    "    #    default=None, metadata={\"help\": \"Dimension of linear layer\"}\n",
    "    #)\n",
    "    # seq_len: int = field(default = 128)\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Where do you want to store the pretrained models downloaded from s3\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_a: bool = field(default=False, metadata={\"help\": \"freeze model a\"})\n",
    "    freeze_b: bool = field(default=False, metadata={\"help\": \"freeze model b\"})\n",
    "    num_labels: int = field(default=3)\n",
    "    batch_size: int = field(default=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 128))\n",
    "        self.dense = nn.Linear(4096, len(config.id2label))\n",
    "                \n",
    "    def forward(self, features):\n",
    "        features = self.pool(features)\n",
    "        print(features.shape)\n",
    "        features = features.view(features.shape[0]//4, -1)\n",
    "        features = self.dense(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseTransformer(nn.Module):\n",
    "#     def __init__(self, args, config):\n",
    "#         super(SiameseTransformer, self).__init__()\n",
    "#         self.args = args\n",
    "#         self.model_a = AutoModelForSequenceClassification.from_pretrained(self.args.model_name, \n",
    "#                            config=config, cache_dir=self.args.cache_dir)\n",
    "#         self.loss_fct = nn.CrossEntropyLoss()\n",
    "#         #self.cls = PredictionHeadTransform(config)\n",
    "#         self.cls = nn.Linear(512, len(config.id2label))\n",
    "#         # self.pool = nn.AdaptiveAvgPool2d((args.batch_size, len(config.id2label)))\n",
    "#         #if self.args.freeze_a:\n",
    "#         #    logger.info(\"**** Freezing Model A ****\")\n",
    "#         #    for param in self.model_a.encoder.parameters():\n",
    "#         #        param.requires_grad = False\n",
    "\n",
    "#         #if self.args.freeze_b:\n",
    "#         #    logger.info(\"**** Freezing Model B ****\")\n",
    "#         #    for param in self.model_b.encoder.parameters():\n",
    "#         #        param.requires_grad = False\n",
    "    \n",
    "#     def forward(self, a, b):\n",
    "#         #labels = input_a['labels']\n",
    "#         #input_a.pop('labels')\n",
    "#         #input_b.pop('labels')\n",
    "#         output_a = self.model_a(**a) # [bs, seq_len, 768]\n",
    "#         output_b = self.model_a(**b)\n",
    "#         outputs = []\n",
    "#         outputs.append(output_a[0]+output_b[0])\n",
    "#         concat_output = torch.cat([output_a[1], output_b[1]])\n",
    "#         concat_output = concat_output.view(3, -1)\n",
    "#         logits = self.cls(concat_output)\n",
    "#         outputs.append(logits)\n",
    "#         #loss = self.loss_fct(logits, labels)\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SiameseModelArguments('bert-base-uncased', batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81381254233a4ed4b1e81ecfece6e438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels = 3,\n",
    "        task_name = 'MNLI',\n",
    "        cache_dir = '/home/nlp/experiments/siamese'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = PredictionHeadTransform(config)\n",
    "# output = torch.rand(8, 64, 768) # 8, 3\n",
    "# head(output).shape\n",
    "# head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, args, config):\n",
    "        super(SiameseTransformer, self).__init__()\n",
    "        self.args = args\n",
    "        self.model_a = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.args.model_name, config=config, cache_dir=self.args.cache_dir\n",
    "        )\n",
    "        self.model_b = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.args.model_name, config=config, cache_dir=self.args.cache_dir\n",
    "        )\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        # self.cls = PredictionHeadTransform(config)\n",
    "        # self.cls = nn.Linear(len(config.id2label), len(config.id2label))\n",
    "        # if self.args.freeze_a:\n",
    "        #    logger.info(\"**** Freezing Model A ****\")\n",
    "        #    for param in self.model_a.encoder.parameters():\n",
    "        #        param.requires_grad = False\n",
    "\n",
    "        # if self.args.freeze_b:\n",
    "        #    logger.info(\"**** Freezing Model B ****\")\n",
    "        #    for param in self.model_b.encoder.parameters():\n",
    "        #        param.requires_grad = False\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        # labels = input_a['labels']\n",
    "        # input_a.pop('labels')\n",
    "        # input_b.pop('labels')\n",
    "        output_a = self.model_a(**a)  # [bs, seq_len, 768]\n",
    "        output_b = self.model_b(**b)\n",
    "        outputs = []\n",
    "        for i in range(len(output_a)):\n",
    "            outputs.append(output_a[i] + output_b[i])\n",
    "\n",
    "        # concat_output = torch.cat([output_a[1], output_b[1]])\n",
    "        # logits = self.cls(concat_output)\n",
    "        # outputs.append(logits)\n",
    "        # loss = self.loss_fct(logits, labels)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SiameseTransformer(args, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_eval = next(iter(eval_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in batch['a'].items():\n",
    "    batch['a'][k] = v.cuda()\n",
    "for k, v in batch['b'].items():\n",
    "    batch['b'][k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/home/nlp/experiments/siamese/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(ckpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.1629, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " tensor([[ 0.1034, -0.1250, -0.1374],\n",
       "         [-0.2326, -0.4631,  0.0924],\n",
       "         [-0.5709, -0.4091,  0.1518],\n",
       "         [-0.4632, -0.3334,  0.1755],\n",
       "         [-0.0604, -0.2989,  0.0755],\n",
       "         [-0.5815, -0.2825,  0.1942],\n",
       "         [-0.2083, -0.3863,  0.1067],\n",
       "         [-0.2706, -0.2518, -0.0087]], device='cuda:0', grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0891, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, glue_compute_metrics\n",
    "from core.siamese_trainer import SiameseTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = '/home/nlp/experiments/siamese/',\n",
    "                                 do_eval = True,\n",
    "                                 per_device_train_batch_size=1024,\n",
    "                                 per_device_eval_batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = \"classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/calvin/huggingface\" target=\"_blank\">https://app.wandb.ai/calvin/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/calvin/huggingface/runs/20arnzdf\" target=\"_blank\">https://app.wandb.ai/calvin/huggingface/runs/20arnzdf</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error uploading \"wandb-metadata.json\": CommError, /tmp/tmp_g8zo58zwandb/1uh8tge9-wandb-metadata.json is an empty file\n"
     ]
    }
   ],
   "source": [
    "def build_compute_metrics_fn(task_name: str,) -> Callable[[EvalPrediction], Dict]:\n",
    "    def compute_metrics_fn(p: EvalPrediction) -> Dict:\n",
    "        if output_mode == \"classification\":\n",
    "            preds = np.argmax(p.predictions, axis=1)\n",
    "        elif output_mode == \"regression\":\n",
    "            preds = np.squeeze(p.predictions)\n",
    "        return glue_compute_metrics(data_args.task_name, preds, p.label_ids)\n",
    "\n",
    "    return compute_metrics_fn\n",
    "\n",
    "trainer = SiameseTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=siamese_train_dataset,\n",
    "    eval_dataset=siamese_eval_dataset,\n",
    "    data_collator=siamese_data_collator,\n",
    "    compute_metrics=build_compute_metrics_fn(data_args.task_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.build_compute_metrics_fn.<locals>.compute_metrics_fn(p: transformers.trainer_utils.EvalPrediction) -> Dict>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_compute_metrics_fn('mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_siamese_eval_dataset = SiameseGlueDataset(data_args, tokenizer, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/5 [00:00<?, ?it/s]/home/nlp/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:25<00:00,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9815 9815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.286630964279175, 'eval_mnli/acc': 0.32786551197147223}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(mm_siamese_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9815"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(siamese_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = AutoModel.from_pretrained('bert-base-uncased', \n",
    "                           config=config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['a'].pop('labels')\n",
    "output_a = model_a(**batch['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
