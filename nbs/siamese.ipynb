{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Optional, Dict, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification, AutoModel,\n",
    "                          AutoTokenizer, PreTrainedTokenizer, PreTrainedModel)\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import TrainingArguments, default_data_collator, EvalPrediction\n",
    "from datasets.siamese_dataset import SiameseGlueDataset, siamese_data_collator\n",
    "from models.siamese_model import SiameseTransformer\n",
    "from core.siamese_trainer import SiameseTrainer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'mnli'\n",
    "data_dir = '/home/nlp/data/glue_data/MNLI'\n",
    "model_id = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = DataTrainingArguments(task_name, data_dir = data_dir, max_seq_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_train_dataset = SiameseGlueDataset(data_args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_eval_dataset = SiameseGlueDataset(data_args, tokenizer, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.siamese_dataset import siamese_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(siamese_train_dataset,\n",
    "                      batch_size=8,\n",
    "                     collate_fn = siamese_data_collator, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dl = DataLoader(siamese_train_dataset,\n",
    "                      batch_size=8,\n",
    "                     collate_fn = siamese_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SiameseModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to SiameseTransformer\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = field(\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Path to pretrained model or model identifier from\"\n",
    "                \" huggingface.co/models\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    #input_dim: int = field(\n",
    "    #    default=None, metadata={\"help\": \"Input dimension of linear layer\"}\n",
    "    #)\n",
    "    #linear_dim: int = field(\n",
    "    #    default=None, metadata={\"help\": \"Dimension of linear layer\"}\n",
    "    #)\n",
    "    seq_len: int = field(default = 128)\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Where do you want to store the pretrained models downloaded from s3\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_a: bool = field(default=False, metadata={\"help\": \"freeze model a\"})\n",
    "    freeze_b: bool = field(default=False, metadata={\"help\": \"freeze model b\"})\n",
    "    num_labels: int = field(default=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 128))\n",
    "        self.dense = nn.Linear(4096, len(config.id2label))\n",
    "                \n",
    "    def forward(self, features):\n",
    "        features = self.pool(features)\n",
    "        print(features.shape)\n",
    "        features = features.view(features.shape[0]//4, -1)\n",
    "        features = self.dense(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, args, config):\n",
    "        super(SiameseTransformer, self).__init__()\n",
    "        self.args = args\n",
    "        self.model_a = AutoModel.from_pretrained(self.args.model_name, \n",
    "                           config=config, cache_dir=self.args.cache_dir)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        self.cls = PredictionHeadTransform(config)\n",
    "        \n",
    "        if self.args.freeze_a:\n",
    "            logger.info(\"**** Freezing Model A ****\")\n",
    "            for param in self.model_a.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.args.freeze_b:\n",
    "            logger.info(\"**** Freezing Model B ****\")\n",
    "            for param in self.model_b.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        labels = input_a['labels']\n",
    "        input_a.pop('labels')\n",
    "        input_b.pop('labels')\n",
    "        output_a = self.model_a(**input_a)[0] # [bs, seq_len, 768]\n",
    "        output_b = self.model_a(**input_b)[0]\n",
    "        concat_output = torch.cat([output_a, output_b, (output_a-output_b), (output_a*output_b)])\n",
    "        logits = self.cls(concat_output)\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, args, config):\n",
    "        super(SiameseTransformer, self).__init__()\n",
    "        self.args = args\n",
    "        self.model_a = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.args.model_name, config=config, cache_dir=self.args.cache_dir\n",
    "        )\n",
    "        # self.model_b = AutoModel.from_pretrained(self.args.model_name,\n",
    "        #                   config=config, cache_dir=self.args.cache_dir)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        self.cls = PredictionHeadTransform(config)\n",
    "\n",
    "        if self.args.freeze_a:\n",
    "            logger.info(\"**** Freezing Model A ****\")\n",
    "            for param in self.model_a.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # if self.args.freeze_b:\n",
    "        #    logger.info(\"**** Freezing Model B ****\")\n",
    "        #    for param in self.model_b.encoder.parameters():\n",
    "        #        param.requires_grad = False\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        #labels = input_a[\"labels\"]\n",
    "        #input_a = inputs[\"a\"]\n",
    "        #input_b = inputs[\"b\"]\n",
    "        #input_a.pop(\"labels\")\n",
    "        #input_b.pop(\"labels\")\n",
    "        #output_a = self.model_a(**input_a)[0]  # [bs, seq_len, 768]\n",
    "        #output_b = self.model_a(**input_b)[0]\n",
    "        #concat_output = torch.cat(\n",
    "        #    [output_a, output_b, (output_a - output_b), (output_a * output_b)]\n",
    "        #)\n",
    "        #logits = self.cls(concat_output)\n",
    "        #loss = self.loss_fct(logits, labels)\n",
    "        # print(a)\n",
    "        output = self.model_a(**a)\n",
    "        return output\n",
    "        #return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SiameseModelArguments('bert-base-uncased', seq_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels = 3,\n",
    "        task_name = 'MNLI',\n",
    "        cache_dir = '/home/nlp/experiments/siamese'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = PredictionHeadTransform(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.rand(8, 64, 768) # 8, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionHeadTransform(\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(8, 128))\n",
       "  (dense): Linear(in_features=4096, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SiameseTransformer(args, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_eval = next(iter(eval_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs in eval_dl:\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a {'labels': tensor([2, 1, 1, 1, 2, 1, 2, 1]), 'input_ids': tensor([[  101, 17158,  2135,  6949,  8301, 25057,  2038,  2048,  3937,  9646,\n",
      "          1011,  4031,  1998, 10505,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2017,  2113,  2076,  1996,  2161,  1998,  1045,  3984,  2012,\n",
      "          2012,  2115,  2504,  7910,  2017,  4558,  2068,  2000,  1996,  2279,\n",
      "          2504,  2065,  2065,  2027,  5630,  2000,  9131,  1996,  1996,  6687,\n",
      "          2136,   102],\n",
      "        [  101,  2028,  1997,  2256,  2193,  2097,  4287,  2041,  2115,  8128,\n",
      "          3371,  2135,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2129,  2079,  2017,  2113,  1029,  2035,  2023,  2003,  2037,\n",
      "          2592,  2153,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  3398,  1045,  2425,  2017,  2054,  2295,  2065,  2017,  2175,\n",
      "          3976,  2070,  1997,  2216,  5093,  6007,  1045,  2064,  2156,  2339,\n",
      "          2085,  2017,  2113,  2027,  1005,  2128,  2893,  2039,  1999,  1996,\n",
      "          3634,   102],\n",
      "        [  101,  2026,  3328,  2386,  3631,  2061,  1045,  1005,  1049,  6314,\n",
      "          2085,  1045,  2074,  2031,  2000,  2735,  1996, 12991,  2039,  2613,\n",
      "          5189,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2021,  1037,  2261,  3017, 16061,  2015,  5788,  2682,  1996,\n",
      "          9706,  3366,  2003,  1996,  6261,  2007,  1996, 10527,  4441,  1010,\n",
      "          2007,  1996, 28185,  6127,  2000,  1996,  2157,  1006,  2010,  7452,\n",
      "          2745,   102],\n",
      "        [  101,  1006,  3191,  2005, 12796,  1005,  1055,  2202,  2006,  4027,\n",
      "          1005,  1055,  9556,  1012,  1007,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "b {'labels': tensor([2, 1, 1, 1, 2, 1, 2, 1]), 'input_ids': tensor([[  101,  4031,  1998, 10505,  2024,  2054,  2191,  6949,  8301, 25057,\n",
      "          2147,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2017,  4558,  1996,  2477,  2000,  1996,  2206,  2504,  2065,\n",
      "          1996,  2111,  9131,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1037,  2266,  1997,  2026,  2136,  2097, 15389,  2115,  4449,\n",
      "          2007, 14269, 11718,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2023,  2592,  7460,  2000,  2068,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1996,  5093,  6007,  2031,  1037,  2846,  1997,  7597,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  1005,  1049,  6314,  2008,  2026,  3328,  2386,  3631,\n",
      "          1998,  2085,  1045,  2031,  2000,  2735,  1996, 12991,  2039,  2428,\n",
      "          5189,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2087,  1997,  1996,  3017, 16061,  2015,  2020,  3908,  2011,\n",
      "          7486,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 12796,  2018,  2019,  5448,  2006,  4027,  1005,  1055,  9556,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for inputs in eval_dl:\n",
    "    for k, v in inputs.items():\n",
    "        print(k, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in batch['a'].items():\n",
    "    batch['a'][k] = v.cuda()\n",
    "for k, v in batch['b'].items():\n",
    "    batch['b'][k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in batch_eval['a'].items():\n",
    "    batch_eval['a'][k] = v.cuda()\n",
    "for k, v in batch_eval['b'].items():\n",
    "    batch_eval['b'][k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.2290, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " tensor([[ 0.0624, -0.1528, -0.0248],\n",
       "         [-0.4474, -0.3496,  0.0997],\n",
       "         [-0.3453, -0.3189,  0.1031],\n",
       "         [-0.3425, -0.3321,  0.0577],\n",
       "         [-0.3478, -0.2492, -0.0216],\n",
       "         [-0.3940, -0.3169,  0.0230],\n",
       "         [-0.5347, -0.2935,  0.1017],\n",
       "         [-0.5581, -0.2490, -0.0911]], device='cuda:0', grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.1629, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " tensor([[ 0.1034, -0.1250, -0.1374],\n",
       "         [-0.2326, -0.4631,  0.0924],\n",
       "         [-0.5709, -0.4091,  0.1518],\n",
       "         [-0.4632, -0.3334,  0.1755],\n",
       "         [-0.0604, -0.2989,  0.0755],\n",
       "         [-0.5815, -0.2825,  0.1942],\n",
       "         [-0.2083, -0.3863,  0.1067],\n",
       "         [-0.2706, -0.2518, -0.0087]], device='cuda:0', grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9698, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, glue_compute_metrics\n",
    "from core.siamese_trainer import SiameseTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = '/home/nlp/experiments/siamese/',\n",
    "                                 do_eval = True,\n",
    "                                 per_device_train_batch_size=1024,\n",
    "                                 per_device_eval_batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = \"classification\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/calvin/huggingface\" target=\"_blank\">https://app.wandb.ai/calvin/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/calvin/huggingface/runs/u0p7hqmb\" target=\"_blank\">https://app.wandb.ai/calvin/huggingface/runs/u0p7hqmb</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_compute_metrics_fn(task_name: str,) -> Callable[[EvalPrediction], Dict]:\n",
    "    def compute_metrics_fn(p: EvalPrediction) -> Dict:\n",
    "        if output_mode == \"classification\":\n",
    "            preds = np.argmax(p.predictions, axis=1)\n",
    "        elif output_mode == \"regression\":\n",
    "            preds = np.squeeze(p.predictions)\n",
    "        return glue_compute_metrics(data_args.task_name, preds, p.label_ids)\n",
    "\n",
    "    return compute_metrics_fn\n",
    "\n",
    "trainer = SiameseTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=siamese_train_dataset,\n",
    "    eval_dataset=siamese_eval_dataset,\n",
    "    data_collator=siamese_data_collator,\n",
    "    compute_metrics=build_compute_metrics_fn(data_args.task_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.build_compute_metrics_fn.<locals>.compute_metrics_fn(p: transformers.trainer_utils.EvalPrediction) -> Dict>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_compute_metrics_fn('mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 5/5 [00:02<00:00,  1.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1113010883331298, 'eval_mnli/acc': 0.31981660723382577}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9815"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(siamese_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = AutoModel.from_pretrained('bert-base-uncased', \n",
    "                           config=config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['a'].pop('labels')\n",
    "output_a = model_a(**batch['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
