{"format": "torch", "nodes": [{"name": "albert", "id": 139660034165672, "class_name": "AlbertModel(\n  (embeddings): AlbertEmbeddings(\n    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): AlbertTransformer(\n    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n    (albert_layer_groups): ModuleList(\n      (0): AlbertLayerGroup(\n        (albert_layers): ModuleList(\n          (0): AlbertLayer(\n            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (attention): AlbertAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n          )\n        )\n      )\n    )\n  )\n  (pooler): Linear(in_features=768, out_features=768, bias=True)\n  (pooler_activation): Tanh()\n)", "parameters": [["embeddings.word_embeddings.weight", [30000, 128]], ["embeddings.position_embeddings.weight", [512, 128]], ["embeddings.token_type_embeddings.weight", [2, 128]], ["embeddings.LayerNorm.weight", [128]], ["embeddings.LayerNorm.bias", [128]], ["encoder.embedding_hidden_mapping_in.weight", [768, 128]], ["encoder.embedding_hidden_mapping_in.bias", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight", [768, 768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight", [768, 768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight", [768, 768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight", [768, 768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias", [768]], ["encoder.albert_layer_groups.0.albert_layers.0.ffn.weight", [3072, 768]], ["encoder.albert_layer_groups.0.albert_layers.0.ffn.bias", [3072]], ["encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight", [768, 3072]], ["encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias", [768]], ["pooler.weight", [768, 768]], ["pooler.bias", [768]]], "output_shape": [[32, 128, 768], [32, 768]], "num_parameters": [3840000, 65536, 256, 128, 128, 98304, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 589824, 768]}, {"name": "dropout", "id": 139660034129368, "class_name": "Dropout(p=0.1, inplace=False)", "parameters": [], "output_shape": [[32, 768]], "num_parameters": []}, {"name": "classifier", "id": 139660034129816, "class_name": "Linear(in_features=768, out_features=3, bias=True)", "parameters": [["weight", [3, 768]], ["bias", [3]]], "output_shape": [[32, 3]], "num_parameters": [2304, 3]}], "edges": []}