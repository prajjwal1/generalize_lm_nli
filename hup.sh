for epoch in 4 5 6 7; do python3 train_siamese.py   --model_name bert-large-uncased --task_name MNLI --do_eval   --data_dir /home/nlp/data/glue_data/MNLI   --max_seq_length 64   --per_device_train_batch_size 128  --learning_rate 2e-5  --num_train_epochs 1.0   --output_dir /home/nlp/experiments/siamese/bert_large/"epoch_"$((epoch+1)) --per_device_eval_batch_size 128 --do_train --fp16 --config_name bert-large-uncased --tokenizer_name bert-large-uncased --model_weights_path /home/nlp/experiments/siamese/bert_large/"epoch_"$epoch --overwrite_output_dir; done
